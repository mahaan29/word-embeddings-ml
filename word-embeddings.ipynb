{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b9540e1a-6235-4c20-9441-820f3a10b032",
   "metadata": {},
   "source": [
    "# CPSC 330 - Applied Machine Learning \n",
    "\n",
    "## Homework 8: Word embeddings, time series\n",
    "### Associated lectures: Lectures 16, 18, 19\n",
    "\n",
    "**Due date: Tuesday, June 21, 2022 at 18:00**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e851ce52-f981-4c4e-9e9e-a740edd12e41",
   "metadata": {},
   "source": [
    "## Table of Contents\n",
    "\n",
    "- [Submission instructions](#sg)\n",
    "- [Exercise 1 - Exploring pre-trained word embeddings](#1)\n",
    "- [Exercise 2 - Exploring time series data](#2)\n",
    "- [Exercise 3 - Short answer questions](#3)\n",
    "- (Optional)[Exercise 4 - Course take away](#4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "a4651888-484b-42a0-95e1-d273e5069205",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from sklearn.cluster import DBSCAN, KMeans\n",
    "from sklearn.compose import ColumnTransformer, make_column_transformer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.model_selection import (\n",
    "    GridSearchCV,\n",
    "    RandomizedSearchCV,\n",
    "    cross_validate,\n",
    "    train_test_split,\n",
    ")\n",
    "from sklearn.pipeline import Pipeline, make_pipeline\n",
    "from sklearn.preprocessing import OneHotEncoder, OrdinalEncoder, StandardScaler\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "pd.set_option(\"display.max_colwidth\", 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "086914c2-5de1-414a-8770-23bef9f312d0",
   "metadata": {},
   "source": [
    "<br><br><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afe22cb5-f825-4dba-b5e3-3538f4afe703",
   "metadata": {},
   "source": [
    "## Instructions \n",
    "<hr>\n",
    "rubric={points:1}\n",
    "\n",
    "Follow the [homework submission instructions](https://github.com/UBC-CS/cpsc330-2022s/blob/master/docs/homework_instructions.md).\n",
    "\n",
    "**You may work with a partner on this homework and submit your assignment as a group.** Below are some instructions on working as a group.\n",
    "- The maximum group size is 2.\n",
    "- Use group work as an opportunity to collaborate and learn new things from each other. \n",
    "- Be respectful to each other and make sure you understand all the concepts in the assignment well. \n",
    "- It's your responsibility to make sure that the assignment is submitted by one of the group members before the deadline. \n",
    "- You can find the instructions on how to do group submission on Gradescope [here](https://help.gradescope.com/article/m5qz2xsnjy-student-add-group-members)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69be5b2d-1854-4c63-bcc6-9b6258b7293a",
   "metadata": {},
   "source": [
    "<br><br><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "859b3f00-a3e5-45d8-b504-22a84ace38cd",
   "metadata": {},
   "source": [
    "## Exercise 1:  Exploring pre-trained word embeddings <a name=\"1\"></a>\n",
    "<hr>\n",
    "\n",
    "In lecture 16, we talked about natural language processing (NLP). Using pre-trained word embeddings is very common in NLP. It has been shown that pre-trained word embeddings [work well on a variety of text classification tasks](http://www.lrec-conf.org/proceedings/lrec2018/pdf/721.pdf). These embeddings are created by training a model like Word2Vec on a huge corpus of text such as a dump of Wikipedia or a dump of the web crawl. \n",
    "\n",
    "A number of pre-trained word embeddings are available out there. Some popular ones are: \n",
    "\n",
    "- [GloVe](https://nlp.stanford.edu/projects/glove/)\n",
    "    * trained using [the GloVe algorithm](https://nlp.stanford.edu/pubs/glove.pdf) \n",
    "    * published by Stanford University \n",
    "- [fastText pre-trained embeddings for 294 languages](https://fasttext.cc/docs/en/pretrained-vectors.html) \n",
    "    * trained using the fastText algorithm\n",
    "    * published by Facebook\n",
    "    \n",
    "In this exercise, you will be exploring GloVe Wikipedia pre-trained embeddings. The code below loads pre-trained word vectors trained on Wikipedia. The vectors are created using an algorithm called GloVe. To run the code, you'll need `gensim` package in your cpsc330 conda environment, which you can install as follows:\n",
    "\n",
    "`conda install -n cpsc330 -c anaconda gensim`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b4823523-ca44-48a3-94bb-f6e453d27f1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available models to download:\n",
      "fasttext-wiki-news-subwords-300, conceptnet-numberbatch-17-06-300, word2vec-ruscorpora-300, word2vec-google-news-300, glove-wiki-gigaword-50, glove-wiki-gigaword-100, glove-wiki-gigaword-200, glove-wiki-gigaword-300, glove-twitter-25, glove-twitter-50, glove-twitter-100, glove-twitter-200, __testing_word2vec-matrix-synopsis\n"
     ]
    }
   ],
   "source": [
    "import gensim\n",
    "import gensim.downloader\n",
    "\n",
    "print(\"Available models to download:\")\n",
    "print(*gensim.downloader.info()[\"models\"].keys(), sep=\", \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "83e4717e-215b-4c1b-b08a-9f5adbb52467",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This will take a while to run when you run it for the first time.\n",
    "import gensim.downloader as api\n",
    "\n",
    "glove_wiki_vectors = api.load(\"glove-wiki-gigaword-100\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "76ec38c4-ce89-4372-b015-035f4d682132",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "400000"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(glove_wiki_vectors)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c78dafb-f712-447a-b870-1fac6c249e5f",
   "metadata": {},
   "source": [
    "There are 400,000 word vectors in these pre-trained model. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce2a75ac-fd18-4a53-89d3-26f1051c4ef3",
   "metadata": {},
   "source": [
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8119fb78-d2be-4ccf-8c8d-31026563e072",
   "metadata": {},
   "source": [
    "### 1.1 Word similarity using pre-trained embeddings\n",
    "rubric={points:4}\n",
    "\n",
    "Now that we have GloVe Wiki vectors (`glove_wiki_vectors`) loaded, let's explore the word vectors. \n",
    "\n",
    "**Your tasks:**\n",
    "\n",
    "1. Calculate cosine similarity for the following word pairs (`word_pairs`) using the [`similarity`](https://radimrehurek.com/gensim/models/keyedvectors.html?highlight=similarity#gensim.models.keyedvectors.KeyedVectors.similarity) method of the model.\n",
    "2. Do the similarities make sense? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "aadd1aa6-6bb8-48d7-a959-691e19d411ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_pairs = [\n",
    "    (\"coast\", \"shore\"),\n",
    "    (\"clothes\", \"closet\"),\n",
    "    (\"old\", \"new\"),\n",
    "    (\"smart\", \"intelligent\"),\n",
    "    (\"dog\", \"cat\"),\n",
    "    (\"tree\", \"lawyer\"),\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "165ca2a9-eba8-472e-a49b-9d0561d96846",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "454bb01f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cosine simlarity between coast and shore = 0.700\n",
      "Cosine simlarity between clothes and closet = 0.546\n",
      "Cosine simlarity between old and new = 0.643\n",
      "Cosine simlarity between smart and intelligent = 0.755\n",
      "Cosine simlarity between dog and cat = 0.880\n",
      "Cosine simlarity between tree and lawyer = 0.077\n"
     ]
    }
   ],
   "source": [
    "for (pair1, pair2) in word_pairs:\n",
    "    print(\"Cosine simlarity between %s and %s = %0.3f\" % (pair1, pair2, glove_wiki_vectors.similarity(pair1, pair2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4156cd83",
   "metadata": {},
   "source": [
    "The similarities here do make sense. The cosine similarity between tree and lawyer is quite low since these two words are not frequently occuring. The high similarity between dog and cat can be explained by the fact that although these words are not synonyms but they are used in the same context."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0331ffe-bf58-4198-bd1e-3b62a5d34319",
   "metadata": {},
   "source": [
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d528120-c1ff-4203-82b8-404b62ba6bb0",
   "metadata": {},
   "source": [
    "### 1.2 Bias in embeddings\n",
    "rubric={points:10}\n",
    "\n",
    "**Your tasks:**\n",
    "1. In Lecture 16 we saw that our pre-trained word embedding model output an analogy that reinforced a gender stereotype. Give an example of how using such a model could cause harm in the real world.\n",
    "2. Here we are using pre-trained embeddings which are built using Wikipedia data. Explore whether there are any worrisome biases present in these embeddings or not by trying out some examples. You can use the following two methods or other methods of your choice to explore what kind of stereotypes and biases are encoded in these embeddings. \n",
    "    - You can use the `analogy` function below which gives words analogies. \n",
    "    - You can also use [similarity](https://radimrehurek.com/gensim/models/keyedvectors.html?highlight=similarity#gensim.models.keyedvectors.KeyedVectors.similarity) or [distance](https://radimrehurek.com/gensim/models/keyedvectors.html?highlight=distance#gensim.models.keyedvectors.KeyedVectors.distances) methods. (An example is shown below.)   \n",
    "3. Discuss your observations. Do you observe the gender stereotype we observed in class in these embeddings?\n",
    "\n",
    "> Note that most of the recent embeddings are de-biased. But you might still observe some biases in them. Also, not all stereotypes present in pre-trained embeddings are necessarily bad. But you should be aware of them when you use them in your models. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "59f7ef2b-d6b4-4338-a153-77fd4fc9847e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analogy(word1, word2, word3, model=glove_wiki_vectors):\n",
    "    \"\"\"\n",
    "    Returns analogy word using the given model.\n",
    "\n",
    "    Parameters\n",
    "    --------------\n",
    "    word1 : (str)\n",
    "        word1 in the analogy relation\n",
    "    word2 : (str)\n",
    "        word2 in the analogy relation\n",
    "    word3 : (str)\n",
    "        word3 in the analogy relation\n",
    "    model :\n",
    "        word embedding model\n",
    "\n",
    "    Returns\n",
    "    ---------------\n",
    "        pd.dataframe\n",
    "    \"\"\"\n",
    "    print(\"%s : %s :: %s : ?\" % (word1, word2, word3))\n",
    "    sim_words = model.most_similar(positive=[word3, word2], negative=[word1])\n",
    "    return pd.DataFrame(sim_words, columns=[\"Analogy word\", \"Score\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0d900c0-3027-4b9f-a750-bca946cf7bdb",
   "metadata": {},
   "source": [
    "An example of using similarity between words to explore biases and stereotypes.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "13f0bacc-ba70-43e3-a7be-07c263f48048",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.447236"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "glove_wiki_vectors.similarity(\"white\", \"rich\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2d2e6671-c6db-4cc9-9652-faba038e8e50",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.51745194"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "glove_wiki_vectors.similarity(\"black\", \"rich\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bbf431d-a56d-4d78-b381-eee21bceb049",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "97899269",
   "metadata": {},
   "source": [
    "1. Using this model for a hiring algorithm can introduce a gender bias, which is harmful in the real world."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "1f268509",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "men : independent :: women : ?\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Analogy word</th>\n",
       "      <th>Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>nonprofit</td>\n",
       "      <td>0.623269</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>liberal</td>\n",
       "      <td>0.613263</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>society</td>\n",
       "      <td>0.602621</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>established</td>\n",
       "      <td>0.601128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>establishment</td>\n",
       "      <td>0.597300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>conservative</td>\n",
       "      <td>0.595048</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>community</td>\n",
       "      <td>0.594654</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>advocacy</td>\n",
       "      <td>0.591874</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>public</td>\n",
       "      <td>0.585784</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>educational</td>\n",
       "      <td>0.582153</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Analogy word     Score\n",
       "0  nonprofit      0.623269\n",
       "1  liberal        0.613263\n",
       "2  society        0.602621\n",
       "3  established    0.601128\n",
       "4  establishment  0.597300\n",
       "5  conservative   0.595048\n",
       "6  community      0.594654\n",
       "7  advocacy       0.591874\n",
       "8  public         0.585784\n",
       "9  educational    0.582153"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "analogy(\"men\", \"independent\", \"women\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "dded1655",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.2875564, 0.3301347)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "glove_wiki_vectors.similarity(\"men\", \"influential\"), glove_wiki_vectors.similarity(\"women\", \"influential\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "24bbb495",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.4394357, 0.37589207)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "glove_wiki_vectors.similarity(\"man\", \"intelligent\"), glove_wiki_vectors.similarity(\"woman\", \"intelligent\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "6d12ebb0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.24923676, 0.15717775)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "glove_wiki_vectors.similarity(\"man\", \"developer\"), glove_wiki_vectors.similarity(\"woman\", \"developer\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "768990e8",
   "metadata": {},
   "source": [
    "There are not very worrisome word embeddings as compared to what we had observed in the class, which could be partly because the algorithms used for deducing the word embeddings are designed to be non-biased. \n",
    "\n",
    "There is also a probable reason which make the word embeddings to appear biased, which can be explained by the fact that some words which are not used a lot would tend to have a lower similarity, hence a lower score causing some instances to appear more biased."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19f04b87-5fa0-4eb4-bb50-cb21aa7ffd1c",
   "metadata": {},
   "source": [
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e6ccd3e-25c1-413d-b079-043fb2949090",
   "metadata": {},
   "source": [
    "### 1.3 Representation of all words in English\n",
    "rubric={reasoning:1}\n",
    "\n",
    "**Your tasks:**\n",
    "1. The vocabulary size of Wikipedia embeddings is quite large. Do you think it contains **all** words in English language? What would happen if you try to get a word vector that's unlikely to be present in the vocabulary (e.g., the word \"cpsc330\"). "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c17edd5a",
   "metadata": {},
   "source": [
    "I don't think the vocabulary size of Wikipedia could include all the words in the English language. Trying to retrieve a word vector for a word that is unlikely to be present in the vocabulary (e.g., \"cpsc330\") would result in the generation of an error."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d589eeae-c081-4042-8cdc-7dbc85a2c400",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "464a0fbf-3a9c-42b3-b9cc-5dc474bc2804",
   "metadata": {},
   "source": [
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1d3cd04-30c9-43f4-9b07-6443ab4ecd7d",
   "metadata": {},
   "source": [
    "### 1.4 Classification with pre-trained embeddings\n",
    "rubric={points:8}\n",
    "\n",
    "In lecture 16, we saw that you can conveniently get word vectors with `spaCy` with `en_core_web_md` model. In this exercise, you'll use word embeddings in multi-class text classification task. We will use [HappyDB](https://www.kaggle.com/ritresearch/happydb) corpus which contains about 100,000 happy moments classified into 7 categories: *affection, exercise, bonding, nature, leisure, achievement, enjoy_the_moment*. The data was crowd-sourced via [Amazon Mechanical Turk](https://www.mturk.com/). The ground truth label is not available for all examples, and in this homework, we'll only use the examples where ground truth is available (~15,000 examples). \n",
    "\n",
    "- Download the data from [here](https://www.kaggle.com/ritresearch/happydb).\n",
    "- Unzip the file and copy it in the homework directory.\n",
    "\n",
    "The code below reads the data CSV (assuming that it's present in the current directory as *cleaned_hm.csv*),  cleans it up a bit, and splits it into train and test splits. \n",
    "\n",
    "**Your tasks:**\n",
    "\n",
    "1. Train logistic regression with bag-of-words features and show classification report on the test set. \n",
    "2. Train logistic regression with average embedding representation extracted using spaCy and show classification report on the test set. (You can find an example of extracting average embedding features using spaCy in [lecture 16](https://github.com/UBC-CS/cpsc330-2022s/blob/master/lectures/16_natural-language-processing.ipynb#sentiment-classification-using-average-embeddings#) under *sentiment classification using average embeddings*.)\n",
    "3. Discuss your results. Which model is performing well. Which model would be more interpretable?  \n",
    "4. Are you observing any benefits of transfer learning here? Briefly discuss. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "b7b35845-7976-4cda-b798-0c0700868fea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>wid</th>\n",
       "      <th>reflection_period</th>\n",
       "      <th>original_hm</th>\n",
       "      <th>cleaned_hm</th>\n",
       "      <th>modified</th>\n",
       "      <th>num_sentence</th>\n",
       "      <th>ground_truth_category</th>\n",
       "      <th>predicted_category</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>hmid</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>27676</th>\n",
       "      <td>206</td>\n",
       "      <td>24h</td>\n",
       "      <td>We had a serious talk with some friends of ours who have been flaky lately. They understood and we had a good evening hanging out.</td>\n",
       "      <td>We had a serious talk with some friends of ours who have been flaky lately. They understood and we had a good evening hanging out.</td>\n",
       "      <td>True</td>\n",
       "      <td>2</td>\n",
       "      <td>bonding</td>\n",
       "      <td>bonding</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27678</th>\n",
       "      <td>45</td>\n",
       "      <td>24h</td>\n",
       "      <td>I meditated last night.</td>\n",
       "      <td>I meditated last night.</td>\n",
       "      <td>True</td>\n",
       "      <td>1</td>\n",
       "      <td>leisure</td>\n",
       "      <td>leisure</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27697</th>\n",
       "      <td>498</td>\n",
       "      <td>24h</td>\n",
       "      <td>My grandmother start to walk from the bed after a long time.</td>\n",
       "      <td>My grandmother start to walk from the bed after a long time.</td>\n",
       "      <td>True</td>\n",
       "      <td>1</td>\n",
       "      <td>affection</td>\n",
       "      <td>affection</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27705</th>\n",
       "      <td>5732</td>\n",
       "      <td>24h</td>\n",
       "      <td>I picked my daughter up from the airport and we have a fun and good conversation on the way home.</td>\n",
       "      <td>I picked my daughter up from the airport and we have a fun and good conversation on the way home.</td>\n",
       "      <td>True</td>\n",
       "      <td>1</td>\n",
       "      <td>bonding</td>\n",
       "      <td>affection</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27715</th>\n",
       "      <td>2272</td>\n",
       "      <td>24h</td>\n",
       "      <td>when i received flowers from my best friend</td>\n",
       "      <td>when i received flowers from my best friend</td>\n",
       "      <td>True</td>\n",
       "      <td>1</td>\n",
       "      <td>bonding</td>\n",
       "      <td>bonding</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        wid reflection_period  \\\n",
       "hmid                            \n",
       "27676  206   24h                \n",
       "27678  45    24h                \n",
       "27697  498   24h                \n",
       "27705  5732  24h                \n",
       "27715  2272  24h                \n",
       "\n",
       "                                                                                                                              original_hm  \\\n",
       "hmid                                                                                                                                        \n",
       "27676  We had a serious talk with some friends of ours who have been flaky lately. They understood and we had a good evening hanging out.   \n",
       "27678  I meditated last night.                                                                                                              \n",
       "27697  My grandmother start to walk from the bed after a long time.                                                                         \n",
       "27705  I picked my daughter up from the airport and we have a fun and good conversation on the way home.                                    \n",
       "27715  when i received flowers from my best friend                                                                                          \n",
       "\n",
       "                                                                                                                               cleaned_hm  \\\n",
       "hmid                                                                                                                                        \n",
       "27676  We had a serious talk with some friends of ours who have been flaky lately. They understood and we had a good evening hanging out.   \n",
       "27678  I meditated last night.                                                                                                              \n",
       "27697  My grandmother start to walk from the bed after a long time.                                                                         \n",
       "27705  I picked my daughter up from the airport and we have a fun and good conversation on the way home.                                    \n",
       "27715  when i received flowers from my best friend                                                                                          \n",
       "\n",
       "       modified  num_sentence ground_truth_category predicted_category  \n",
       "hmid                                                                    \n",
       "27676  True      2             bonding               bonding            \n",
       "27678  True      1             leisure               leisure            \n",
       "27697  True      1             affection             affection          \n",
       "27705  True      1             bonding               affection          \n",
       "27715  True      1             bonding               bonding            "
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"cleaned_hm.csv\", index_col=0)\n",
    "sample_df = df.dropna()\n",
    "sample_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "384fa13b-83a5-4e23-9280-c4e529937143",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yXKa7qfQXYPD",
    "outputId": "8bbf5eeb-0151-4853-a49c-3876279bbeb7"
   },
   "outputs": [],
   "source": [
    "sample_df = sample_df.rename(\n",
    "    columns={\"cleaned_hm\": \"moment\", \"ground_truth_category\": \"target\"}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "de04d594-7174-409d-a9fa-c2fd738e2208",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df, test_df = train_test_split(sample_df, test_size=0.3, random_state=123)\n",
    "X_train, y_train = train_df[\"moment\"], train_df[\"target\"]\n",
    "X_test, y_test = test_df[\"moment\"], test_df[\"target\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "db16934a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting spacy\n",
      "  Downloading spacy-3.3.1-cp310-cp310-macosx_10_9_x86_64.whl (6.6 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.6/6.6 MB\u001b[0m \u001b[31m21.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting catalogue<2.1.0,>=2.0.6\n",
      "  Downloading catalogue-2.0.7-py3-none-any.whl (17 kB)\n",
      "Collecting cymem<2.1.0,>=2.0.2\n",
      "  Downloading cymem-2.0.6-cp310-cp310-macosx_10_9_x86_64.whl (32 kB)\n",
      "Collecting thinc<8.1.0,>=8.0.14\n",
      "  Downloading thinc-8.0.17-cp310-cp310-macosx_10_9_x86_64.whl (648 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m648.7/648.7 kB\u001b[0m \u001b[31m12.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: tqdm<5.0.0,>=4.38.0 in /Users/mahaanjain/opt/miniconda3/envs/cpsc330/lib/python3.10/site-packages (from spacy) (4.64.0)\n",
      "Collecting pydantic!=1.8,!=1.8.1,<1.9.0,>=1.7.4\n",
      "  Downloading pydantic-1.8.2-py3-none-any.whl (126 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m126.0/126.0 kB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting preshed<3.1.0,>=3.0.2\n",
      "  Downloading preshed-3.0.6-cp310-cp310-macosx_10_9_x86_64.whl (107 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m107.1/107.1 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting wasabi<1.1.0,>=0.9.1\n",
      "  Downloading wasabi-0.9.1-py3-none-any.whl (26 kB)\n",
      "Collecting spacy-legacy<3.1.0,>=3.0.9\n",
      "  Downloading spacy_legacy-3.0.9-py2.py3-none-any.whl (20 kB)\n",
      "Collecting typer<0.5.0,>=0.3.0\n",
      "  Downloading typer-0.4.1-py3-none-any.whl (27 kB)\n",
      "Collecting spacy-loggers<2.0.0,>=1.0.0\n",
      "  Downloading spacy_loggers-1.0.2-py3-none-any.whl (7.2 kB)\n",
      "Requirement already satisfied: setuptools in /Users/mahaanjain/opt/miniconda3/envs/cpsc330/lib/python3.10/site-packages (from spacy) (62.3.1)\n",
      "Collecting srsly<3.0.0,>=2.4.3\n",
      "  Downloading srsly-2.4.3-cp310-cp310-macosx_10_9_x86_64.whl (457 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m458.0/458.0 kB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting blis<0.8.0,>=0.4.0\n",
      "  Downloading blis-0.7.7-cp310-cp310-macosx_10_9_x86_64.whl (5.8 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.8/5.8 MB\u001b[0m \u001b[31m18.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting langcodes<4.0.0,>=3.2.0\n",
      "  Downloading langcodes-3.3.0-py3-none-any.whl (181 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m181.6/181.6 kB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: requests<3.0.0,>=2.13.0 in /Users/mahaanjain/opt/miniconda3/envs/cpsc330/lib/python3.10/site-packages (from spacy) (2.27.1)\n",
      "Collecting murmurhash<1.1.0,>=0.28.0\n",
      "  Downloading murmurhash-1.0.7-cp310-cp310-macosx_10_9_x86_64.whl (18 kB)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/mahaanjain/opt/miniconda3/envs/cpsc330/lib/python3.10/site-packages (from spacy) (21.3)\n",
      "Requirement already satisfied: jinja2 in /Users/mahaanjain/opt/miniconda3/envs/cpsc330/lib/python3.10/site-packages (from spacy) (3.1.2)\n",
      "Collecting pathy>=0.3.5\n",
      "  Downloading pathy-0.6.1-py3-none-any.whl (42 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.8/42.8 kB\u001b[0m \u001b[31m1.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.15.0 in /Users/mahaanjain/opt/miniconda3/envs/cpsc330/lib/python3.10/site-packages (from spacy) (1.22.3)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /Users/mahaanjain/opt/miniconda3/envs/cpsc330/lib/python3.10/site-packages (from packaging>=20.0->spacy) (3.0.9)\n",
      "Requirement already satisfied: smart-open<6.0.0,>=5.0.0 in /Users/mahaanjain/opt/miniconda3/envs/cpsc330/lib/python3.10/site-packages (from pathy>=0.3.5->spacy) (5.2.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /Users/mahaanjain/opt/miniconda3/envs/cpsc330/lib/python3.10/site-packages (from pydantic!=1.8,!=1.8.1,<1.9.0,>=1.7.4->spacy) (4.2.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/mahaanjain/opt/miniconda3/envs/cpsc330/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.3)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /Users/mahaanjain/opt/miniconda3/envs/cpsc330/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy) (2.0.12)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/mahaanjain/opt/miniconda3/envs/cpsc330/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy) (2020.6.20)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /Users/mahaanjain/opt/miniconda3/envs/cpsc330/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy) (1.26.9)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in /Users/mahaanjain/opt/miniconda3/envs/cpsc330/lib/python3.10/site-packages (from typer<0.5.0,>=0.3.0->spacy) (8.1.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/mahaanjain/opt/miniconda3/envs/cpsc330/lib/python3.10/site-packages (from jinja2->spacy) (2.1.1)\n",
      "Installing collected packages: wasabi, murmurhash, cymem, typer, spacy-loggers, spacy-legacy, pydantic, preshed, langcodes, catalogue, blis, srsly, pathy, thinc, spacy\n",
      "  Attempting uninstall: pydantic\n",
      "    Found existing installation: pydantic 1.9.0\n",
      "    Uninstalling pydantic-1.9.0:\n",
      "      Successfully uninstalled pydantic-1.9.0\n",
      "Successfully installed blis-0.7.7 catalogue-2.0.7 cymem-2.0.6 langcodes-3.3.0 murmurhash-1.0.7 pathy-0.6.1 preshed-3.0.6 pydantic-1.8.2 spacy-3.3.1 spacy-legacy-3.0.9 spacy-loggers-1.0.2 srsly-2.4.3 thinc-8.0.17 typer-0.4.1 wasabi-0.9.1\n",
      "Collecting en-core-web-md==3.3.0\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_md-3.3.0/en_core_web_md-3.3.0-py3-none-any.whl (33.5 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m33.5/33.5 MB\u001b[0m \u001b[31m26.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: spacy<3.4.0,>=3.3.0.dev0 in /Users/mahaanjain/opt/miniconda3/envs/cpsc330/lib/python3.10/site-packages (from en-core-web-md==3.3.0) (3.3.1)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /Users/mahaanjain/opt/miniconda3/envs/cpsc330/lib/python3.10/site-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-md==3.3.0) (2.0.7)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /Users/mahaanjain/opt/miniconda3/envs/cpsc330/lib/python3.10/site-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-md==3.3.0) (2.4.3)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /Users/mahaanjain/opt/miniconda3/envs/cpsc330/lib/python3.10/site-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-md==3.3.0) (4.64.0)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /Users/mahaanjain/opt/miniconda3/envs/cpsc330/lib/python3.10/site-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-md==3.3.0) (1.0.7)\n",
      "Requirement already satisfied: jinja2 in /Users/mahaanjain/opt/miniconda3/envs/cpsc330/lib/python3.10/site-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-md==3.3.0) (3.1.2)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /Users/mahaanjain/opt/miniconda3/envs/cpsc330/lib/python3.10/site-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-md==3.3.0) (2.27.1)\n",
      "Requirement already satisfied: thinc<8.1.0,>=8.0.14 in /Users/mahaanjain/opt/miniconda3/envs/cpsc330/lib/python3.10/site-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-md==3.3.0) (8.0.17)\n",
      "Requirement already satisfied: typer<0.5.0,>=0.3.0 in /Users/mahaanjain/opt/miniconda3/envs/cpsc330/lib/python3.10/site-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-md==3.3.0) (0.4.1)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.4.0 in /Users/mahaanjain/opt/miniconda3/envs/cpsc330/lib/python3.10/site-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-md==3.3.0) (0.7.7)\n",
      "Requirement already satisfied: pathy>=0.3.5 in /Users/mahaanjain/opt/miniconda3/envs/cpsc330/lib/python3.10/site-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-md==3.3.0) (0.6.1)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.9.0,>=1.7.4 in /Users/mahaanjain/opt/miniconda3/envs/cpsc330/lib/python3.10/site-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-md==3.3.0) (1.8.2)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /Users/mahaanjain/opt/miniconda3/envs/cpsc330/lib/python3.10/site-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-md==3.3.0) (3.3.0)\n",
      "Requirement already satisfied: wasabi<1.1.0,>=0.9.1 in /Users/mahaanjain/opt/miniconda3/envs/cpsc330/lib/python3.10/site-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-md==3.3.0) (0.9.1)\n",
      "Requirement already satisfied: numpy>=1.15.0 in /Users/mahaanjain/opt/miniconda3/envs/cpsc330/lib/python3.10/site-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-md==3.3.0) (1.22.3)\n",
      "Requirement already satisfied: setuptools in /Users/mahaanjain/opt/miniconda3/envs/cpsc330/lib/python3.10/site-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-md==3.3.0) (62.3.1)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /Users/mahaanjain/opt/miniconda3/envs/cpsc330/lib/python3.10/site-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-md==3.3.0) (2.0.6)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /Users/mahaanjain/opt/miniconda3/envs/cpsc330/lib/python3.10/site-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-md==3.3.0) (1.0.2)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /Users/mahaanjain/opt/miniconda3/envs/cpsc330/lib/python3.10/site-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-md==3.3.0) (3.0.6)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/mahaanjain/opt/miniconda3/envs/cpsc330/lib/python3.10/site-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-md==3.3.0) (21.3)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.9 in /Users/mahaanjain/opt/miniconda3/envs/cpsc330/lib/python3.10/site-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-md==3.3.0) (3.0.9)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /Users/mahaanjain/opt/miniconda3/envs/cpsc330/lib/python3.10/site-packages (from packaging>=20.0->spacy<3.4.0,>=3.3.0.dev0->en-core-web-md==3.3.0) (3.0.9)\n",
      "Requirement already satisfied: smart-open<6.0.0,>=5.0.0 in /Users/mahaanjain/opt/miniconda3/envs/cpsc330/lib/python3.10/site-packages (from pathy>=0.3.5->spacy<3.4.0,>=3.3.0.dev0->en-core-web-md==3.3.0) (5.2.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /Users/mahaanjain/opt/miniconda3/envs/cpsc330/lib/python3.10/site-packages (from pydantic!=1.8,!=1.8.1,<1.9.0,>=1.7.4->spacy<3.4.0,>=3.3.0.dev0->en-core-web-md==3.3.0) (4.2.0)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /Users/mahaanjain/opt/miniconda3/envs/cpsc330/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.4.0,>=3.3.0.dev0->en-core-web-md==3.3.0) (2.0.12)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/mahaanjain/opt/miniconda3/envs/cpsc330/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.4.0,>=3.3.0.dev0->en-core-web-md==3.3.0) (3.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/mahaanjain/opt/miniconda3/envs/cpsc330/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.4.0,>=3.3.0.dev0->en-core-web-md==3.3.0) (2020.6.20)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /Users/mahaanjain/opt/miniconda3/envs/cpsc330/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.4.0,>=3.3.0.dev0->en-core-web-md==3.3.0) (1.26.9)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in /Users/mahaanjain/opt/miniconda3/envs/cpsc330/lib/python3.10/site-packages (from typer<0.5.0,>=0.3.0->spacy<3.4.0,>=3.3.0.dev0->en-core-web-md==3.3.0) (8.1.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/mahaanjain/opt/miniconda3/envs/cpsc330/lib/python3.10/site-packages (from jinja2->spacy<3.4.0,>=3.3.0.dev0->en-core-web-md==3.3.0) (2.1.1)\n",
      "Installing collected packages: en-core-web-md\n",
      "Successfully installed en-core-web-md-3.3.0\n",
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('en_core_web_md')\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "!{sys.executable} -m pip install spacy\n",
    "!{sys.executable} -m spacy download en_core_web_md"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "73f63a73-0d13-4276-9e79-7ad60575a198",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_md\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f345014-d6cc-46e6-b964-4dd412f46a04",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d3a74dba",
   "metadata": {},
   "source": [
    "1. Logistic regression with bag-of-words features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "08fd7353",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data matrix shape: (9887, 8060)\n"
     ]
    }
   ],
   "source": [
    "pipe = make_pipeline(\n",
    "    CountVectorizer(stop_words=\"english\"), LogisticRegression(max_iter=1000)\n",
    ")\n",
    "pipe.named_steps[\"countvectorizer\"].fit(X_train)\n",
    "X_train_transformed = pipe.named_steps[\"countvectorizer\"].transform(X_train)\n",
    "print(\"Data matrix shape:\", X_train_transformed.shape)\n",
    "pipe.fit(X_train, y_train);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "0195dc0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train accuracy 0.9562051178314959\n",
      "Test accuracy 0.8173666823973572\n"
     ]
    }
   ],
   "source": [
    "print(\"Train accuracy\", pipe.score(X_train, y_train))\n",
    "print(\"Test accuracy\", pipe.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "1d77c5a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                  precision    recall  f1-score   support\n",
      "\n",
      "     achievement       0.79      0.87      0.83      1302\n",
      "       affection       0.90      0.91      0.91      1423\n",
      "         bonding       0.91      0.85      0.88       492\n",
      "enjoy_the_moment       0.60      0.54      0.57       469\n",
      "        exercise       0.91      0.57      0.70        74\n",
      "         leisure       0.73      0.70      0.71       407\n",
      "          nature       0.73      0.46      0.57        71\n",
      "\n",
      "        accuracy                           0.82      4238\n",
      "       macro avg       0.80      0.70      0.74      4238\n",
      "    weighted avg       0.82      0.82      0.81      4238\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test, pipe.predict(X_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7430d12",
   "metadata": {},
   "source": [
    "2. logistic regression with average embedding representation extracted using spaCy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "6572c7d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_emb = pd.DataFrame([text.vector for text in nlp.pipe(X_train)])\n",
    "X_test_emb = pd.DataFrame([text.vector for text in nlp.pipe(X_test)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "3bd75347",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9887, 300)"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_emb.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "d42247a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train accuracy:  0.7279255588146051\n",
      "Test accuracy:  0.7095327984898537\n"
     ]
    }
   ],
   "source": [
    "lr = LogisticRegression(max_iter=1000)\n",
    "lr.fit(X_train_emb, y_train)\n",
    "print(\"Train accuracy: \", lr.score(X_train_emb, y_train))\n",
    "print(\"Test accuracy: \", lr.score(X_test_emb, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "2d2ef9f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                  precision    recall  f1-score   support\n",
      "\n",
      "     achievement       0.73      0.83      0.77      1302\n",
      "       affection       0.72      0.86      0.79      1423\n",
      "         bonding       0.65      0.50      0.57       492\n",
      "enjoy_the_moment       0.56      0.38      0.45       469\n",
      "        exercise       0.78      0.34      0.47        74\n",
      "         leisure       0.76      0.56      0.64       407\n",
      "          nature       0.71      0.42      0.53        71\n",
      "\n",
      "        accuracy                           0.71      4238\n",
      "       macro avg       0.70      0.55      0.60      4238\n",
      "    weighted avg       0.70      0.71      0.70      4238\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test, lr.predict(X_test_emb)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af3939fc",
   "metadata": {},
   "source": [
    "3. The logistic regression with bag of words feature model is performing better and is more interpretable as there will be a coefficient associated with each word, while the logistic regression with average embedding representation does not give a clear interpretation.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa95a153",
   "metadata": {},
   "source": [
    "4. Transfer learning would not be of much use here as it would not improve efficiency if new models were to be trained using this model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec620e19-016a-4476-bb7e-de0c402078d2",
   "metadata": {},
   "source": [
    "## Exercise 2: Exploring time series data <a name=\"2\"></a>\n",
    "<hr>\n",
    "\n",
    "In this exercise we'll be looking at a [dataset of avocado prices](https://www.kaggle.com/neuromusic/avocado-prices). You should start by downloading the dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "b22471fe-942e-49aa-8fb8-9d4fbf6b00b3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>AveragePrice</th>\n",
       "      <th>Total Volume</th>\n",
       "      <th>4046</th>\n",
       "      <th>4225</th>\n",
       "      <th>4770</th>\n",
       "      <th>Total Bags</th>\n",
       "      <th>Small Bags</th>\n",
       "      <th>Large Bags</th>\n",
       "      <th>XLarge Bags</th>\n",
       "      <th>type</th>\n",
       "      <th>year</th>\n",
       "      <th>region</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2015-12-27</td>\n",
       "      <td>1.33</td>\n",
       "      <td>64236.62</td>\n",
       "      <td>1036.74</td>\n",
       "      <td>54454.85</td>\n",
       "      <td>48.16</td>\n",
       "      <td>8696.87</td>\n",
       "      <td>8603.62</td>\n",
       "      <td>93.25</td>\n",
       "      <td>0.0</td>\n",
       "      <td>conventional</td>\n",
       "      <td>2015</td>\n",
       "      <td>Albany</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2015-12-20</td>\n",
       "      <td>1.35</td>\n",
       "      <td>54876.98</td>\n",
       "      <td>674.28</td>\n",
       "      <td>44638.81</td>\n",
       "      <td>58.33</td>\n",
       "      <td>9505.56</td>\n",
       "      <td>9408.07</td>\n",
       "      <td>97.49</td>\n",
       "      <td>0.0</td>\n",
       "      <td>conventional</td>\n",
       "      <td>2015</td>\n",
       "      <td>Albany</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2015-12-13</td>\n",
       "      <td>0.93</td>\n",
       "      <td>118220.22</td>\n",
       "      <td>794.70</td>\n",
       "      <td>109149.67</td>\n",
       "      <td>130.50</td>\n",
       "      <td>8145.35</td>\n",
       "      <td>8042.21</td>\n",
       "      <td>103.14</td>\n",
       "      <td>0.0</td>\n",
       "      <td>conventional</td>\n",
       "      <td>2015</td>\n",
       "      <td>Albany</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2015-12-06</td>\n",
       "      <td>1.08</td>\n",
       "      <td>78992.15</td>\n",
       "      <td>1132.00</td>\n",
       "      <td>71976.41</td>\n",
       "      <td>72.58</td>\n",
       "      <td>5811.16</td>\n",
       "      <td>5677.40</td>\n",
       "      <td>133.76</td>\n",
       "      <td>0.0</td>\n",
       "      <td>conventional</td>\n",
       "      <td>2015</td>\n",
       "      <td>Albany</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2015-11-29</td>\n",
       "      <td>1.28</td>\n",
       "      <td>51039.60</td>\n",
       "      <td>941.48</td>\n",
       "      <td>43838.39</td>\n",
       "      <td>75.78</td>\n",
       "      <td>6183.95</td>\n",
       "      <td>5986.26</td>\n",
       "      <td>197.69</td>\n",
       "      <td>0.0</td>\n",
       "      <td>conventional</td>\n",
       "      <td>2015</td>\n",
       "      <td>Albany</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Date  AveragePrice  Total Volume     4046       4225    4770  \\\n",
       "0 2015-12-27  1.33          64236.62      1036.74  54454.85   48.16    \n",
       "1 2015-12-20  1.35          54876.98      674.28   44638.81   58.33    \n",
       "2 2015-12-13  0.93          118220.22     794.70   109149.67  130.50   \n",
       "3 2015-12-06  1.08          78992.15      1132.00  71976.41   72.58    \n",
       "4 2015-11-29  1.28          51039.60      941.48   43838.39   75.78    \n",
       "\n",
       "   Total Bags  Small Bags  Large Bags  XLarge Bags          type  year  region  \n",
       "0  8696.87     8603.62     93.25       0.0          conventional  2015  Albany  \n",
       "1  9505.56     9408.07     97.49       0.0          conventional  2015  Albany  \n",
       "2  8145.35     8042.21     103.14      0.0          conventional  2015  Albany  \n",
       "3  5811.16     5677.40     133.76      0.0          conventional  2015  Albany  \n",
       "4  6183.95     5986.26     197.69      0.0          conventional  2015  Albany  "
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"avocado.csv\", parse_dates=[\"Date\"], index_col=0)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "3425e59d-9580-4512-8bd2-c8c9b836df8f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(18249, 13)"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "6d0aa8d9-34b6-4401-8b91-77e1342510ab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Timestamp('2015-01-04 00:00:00')"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"Date\"].min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "3b64f1d1-9614-44df-b625-52a77c00af9d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Timestamp('2018-03-25 00:00:00')"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"Date\"].max()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dae5238-ac94-4b1b-b368-d972b6582d8a",
   "metadata": {},
   "source": [
    "It looks like the data ranges from the start of 2015 to March 2018 (~3 years ago), for a total of 3.25 years or so. Let's split the data so that we have a 6 months of test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "5d4d6fb5-1cbb-47e0-a5c4-34b66d026d1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "split_date = \"20170925\"\n",
    "train_df = df[df[\"Date\"] <= split_date]\n",
    "test_df = df[df[\"Date\"] > split_date]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "26a3b6ae-1406-48c3-8b5c-e7b65a041852",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert len(train_df) + len(test_df) == len(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f5ed401-224b-42d3-a4bd-f67b60c3625b",
   "metadata": {},
   "source": [
    "### 2.1\n",
    "rubric={points:4}\n",
    "\n",
    "In the Rain is Australia dataset from lecture, we had different measurements for each Location. What about this dataset: for which categorical feature(s), if any, do we have separate measurements? Justify your answer by referencing the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "4d1cb845",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>AveragePrice</th>\n",
       "      <th>Total Volume</th>\n",
       "      <th>4046</th>\n",
       "      <th>4225</th>\n",
       "      <th>4770</th>\n",
       "      <th>Total Bags</th>\n",
       "      <th>Small Bags</th>\n",
       "      <th>Large Bags</th>\n",
       "      <th>XLarge Bags</th>\n",
       "      <th>type</th>\n",
       "      <th>year</th>\n",
       "      <th>region</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>2015-01-04</td>\n",
       "      <td>1.75</td>\n",
       "      <td>27365.89</td>\n",
       "      <td>9307.34</td>\n",
       "      <td>3844.81</td>\n",
       "      <td>615.28</td>\n",
       "      <td>13598.46</td>\n",
       "      <td>13061.10</td>\n",
       "      <td>537.36</td>\n",
       "      <td>0.0</td>\n",
       "      <td>organic</td>\n",
       "      <td>2015</td>\n",
       "      <td>Southeast</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>2015-01-04</td>\n",
       "      <td>1.49</td>\n",
       "      <td>17723.17</td>\n",
       "      <td>1189.35</td>\n",
       "      <td>15628.27</td>\n",
       "      <td>0.00</td>\n",
       "      <td>905.55</td>\n",
       "      <td>905.55</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>organic</td>\n",
       "      <td>2015</td>\n",
       "      <td>Chicago</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>2015-01-04</td>\n",
       "      <td>1.68</td>\n",
       "      <td>2896.72</td>\n",
       "      <td>161.68</td>\n",
       "      <td>206.96</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2528.08</td>\n",
       "      <td>2528.08</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>organic</td>\n",
       "      <td>2015</td>\n",
       "      <td>HarrisburgScranton</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>2015-01-04</td>\n",
       "      <td>1.52</td>\n",
       "      <td>54956.80</td>\n",
       "      <td>3013.04</td>\n",
       "      <td>35456.88</td>\n",
       "      <td>1561.70</td>\n",
       "      <td>14925.18</td>\n",
       "      <td>11264.80</td>\n",
       "      <td>3660.38</td>\n",
       "      <td>0.0</td>\n",
       "      <td>conventional</td>\n",
       "      <td>2015</td>\n",
       "      <td>Pittsburgh</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>2015-01-04</td>\n",
       "      <td>1.64</td>\n",
       "      <td>1505.12</td>\n",
       "      <td>1.27</td>\n",
       "      <td>1129.50</td>\n",
       "      <td>0.00</td>\n",
       "      <td>374.35</td>\n",
       "      <td>186.67</td>\n",
       "      <td>187.68</td>\n",
       "      <td>0.0</td>\n",
       "      <td>organic</td>\n",
       "      <td>2015</td>\n",
       "      <td>Boise</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         Date  AveragePrice  Total Volume     4046      4225     4770  \\\n",
       "51 2015-01-04  1.75          27365.89      9307.34  3844.81   615.28    \n",
       "51 2015-01-04  1.49          17723.17      1189.35  15628.27  0.00      \n",
       "51 2015-01-04  1.68          2896.72       161.68   206.96    0.00      \n",
       "51 2015-01-04  1.52          54956.80      3013.04  35456.88  1561.70   \n",
       "51 2015-01-04  1.64          1505.12       1.27     1129.50   0.00      \n",
       "\n",
       "    Total Bags  Small Bags  Large Bags  XLarge Bags          type  year  \\\n",
       "51  13598.46    13061.10    537.36      0.0          organic       2015   \n",
       "51  905.55      905.55      0.00        0.0          organic       2015   \n",
       "51  2528.08     2528.08     0.00        0.0          organic       2015   \n",
       "51  14925.18    11264.80    3660.38     0.0          conventional  2015   \n",
       "51  374.35      186.67      187.68      0.0          organic       2015   \n",
       "\n",
       "                region  \n",
       "51  Southeast           \n",
       "51  Chicago             \n",
       "51  HarrisburgScranton  \n",
       "51  Pittsburgh          \n",
       "51  Boise               "
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.sort_values(by=\"Date\").head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a41f8cf3",
   "metadata": {},
   "source": [
    "Measurements made on the same day can be observed at different regression above."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b8d3fbd-8c98-4e3f-8ca5-22571c563d1f",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "aa0e6c59",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>AveragePrice</th>\n",
       "      <th>Total Volume</th>\n",
       "      <th>4046</th>\n",
       "      <th>4225</th>\n",
       "      <th>4770</th>\n",
       "      <th>Total Bags</th>\n",
       "      <th>Small Bags</th>\n",
       "      <th>Large Bags</th>\n",
       "      <th>XLarge Bags</th>\n",
       "      <th>type</th>\n",
       "      <th>year</th>\n",
       "      <th>region</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>2015-01-04</td>\n",
       "      <td>1.22</td>\n",
       "      <td>40873.28</td>\n",
       "      <td>2819.50</td>\n",
       "      <td>28287.42</td>\n",
       "      <td>49.90</td>\n",
       "      <td>9716.46</td>\n",
       "      <td>9186.93</td>\n",
       "      <td>529.53</td>\n",
       "      <td>0.0</td>\n",
       "      <td>conventional</td>\n",
       "      <td>2015</td>\n",
       "      <td>Albany</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>2015-01-04</td>\n",
       "      <td>1.79</td>\n",
       "      <td>1373.95</td>\n",
       "      <td>57.42</td>\n",
       "      <td>153.88</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1162.65</td>\n",
       "      <td>1162.65</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>organic</td>\n",
       "      <td>2015</td>\n",
       "      <td>Albany</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>2015-01-11</td>\n",
       "      <td>1.24</td>\n",
       "      <td>41195.08</td>\n",
       "      <td>1002.85</td>\n",
       "      <td>31640.34</td>\n",
       "      <td>127.12</td>\n",
       "      <td>8424.77</td>\n",
       "      <td>8036.04</td>\n",
       "      <td>388.73</td>\n",
       "      <td>0.0</td>\n",
       "      <td>conventional</td>\n",
       "      <td>2015</td>\n",
       "      <td>Albany</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>2015-01-11</td>\n",
       "      <td>1.77</td>\n",
       "      <td>1182.56</td>\n",
       "      <td>39.00</td>\n",
       "      <td>305.12</td>\n",
       "      <td>0.00</td>\n",
       "      <td>838.44</td>\n",
       "      <td>838.44</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>organic</td>\n",
       "      <td>2015</td>\n",
       "      <td>Albany</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>2015-01-18</td>\n",
       "      <td>1.17</td>\n",
       "      <td>44511.28</td>\n",
       "      <td>914.14</td>\n",
       "      <td>31540.32</td>\n",
       "      <td>135.77</td>\n",
       "      <td>11921.05</td>\n",
       "      <td>11651.09</td>\n",
       "      <td>269.96</td>\n",
       "      <td>0.0</td>\n",
       "      <td>conventional</td>\n",
       "      <td>2015</td>\n",
       "      <td>Albany</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         Date  AveragePrice  Total Volume     4046      4225    4770  \\\n",
       "51 2015-01-04  1.22          40873.28      2819.50  28287.42  49.90    \n",
       "51 2015-01-04  1.79          1373.95       57.42    153.88    0.00     \n",
       "50 2015-01-11  1.24          41195.08      1002.85  31640.34  127.12   \n",
       "50 2015-01-11  1.77          1182.56       39.00    305.12    0.00     \n",
       "49 2015-01-18  1.17          44511.28      914.14   31540.32  135.77   \n",
       "\n",
       "    Total Bags  Small Bags  Large Bags  XLarge Bags          type  year  \\\n",
       "51  9716.46     9186.93     529.53      0.0          conventional  2015   \n",
       "51  1162.65     1162.65     0.00        0.0          organic       2015   \n",
       "50  8424.77     8036.04     388.73      0.0          conventional  2015   \n",
       "50  838.44      838.44      0.00        0.0          organic       2015   \n",
       "49  11921.05    11651.09    269.96      0.0          conventional  2015   \n",
       "\n",
       "    region  \n",
       "51  Albany  \n",
       "51  Albany  \n",
       "50  Albany  \n",
       "50  Albany  \n",
       "49  Albany  "
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.sort_values(by=[\"region\", \"Date\"]).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "304e8610",
   "metadata": {},
   "source": [
    "For the same region, we can observe two measurements which were made on 2015-01-04, which is because there are two different types of avocados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "25f86f68",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>AveragePrice</th>\n",
       "      <th>Total Volume</th>\n",
       "      <th>4046</th>\n",
       "      <th>4225</th>\n",
       "      <th>4770</th>\n",
       "      <th>Total Bags</th>\n",
       "      <th>Small Bags</th>\n",
       "      <th>Large Bags</th>\n",
       "      <th>XLarge Bags</th>\n",
       "      <th>type</th>\n",
       "      <th>year</th>\n",
       "      <th>region</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>2015-01-04</td>\n",
       "      <td>1.22</td>\n",
       "      <td>40873.28</td>\n",
       "      <td>2819.50</td>\n",
       "      <td>28287.42</td>\n",
       "      <td>49.90</td>\n",
       "      <td>9716.46</td>\n",
       "      <td>9186.93</td>\n",
       "      <td>529.53</td>\n",
       "      <td>0.0</td>\n",
       "      <td>conventional</td>\n",
       "      <td>2015</td>\n",
       "      <td>Albany</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>2015-01-11</td>\n",
       "      <td>1.24</td>\n",
       "      <td>41195.08</td>\n",
       "      <td>1002.85</td>\n",
       "      <td>31640.34</td>\n",
       "      <td>127.12</td>\n",
       "      <td>8424.77</td>\n",
       "      <td>8036.04</td>\n",
       "      <td>388.73</td>\n",
       "      <td>0.0</td>\n",
       "      <td>conventional</td>\n",
       "      <td>2015</td>\n",
       "      <td>Albany</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>2015-01-18</td>\n",
       "      <td>1.17</td>\n",
       "      <td>44511.28</td>\n",
       "      <td>914.14</td>\n",
       "      <td>31540.32</td>\n",
       "      <td>135.77</td>\n",
       "      <td>11921.05</td>\n",
       "      <td>11651.09</td>\n",
       "      <td>269.96</td>\n",
       "      <td>0.0</td>\n",
       "      <td>conventional</td>\n",
       "      <td>2015</td>\n",
       "      <td>Albany</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>2015-01-25</td>\n",
       "      <td>1.06</td>\n",
       "      <td>45147.50</td>\n",
       "      <td>941.38</td>\n",
       "      <td>33196.16</td>\n",
       "      <td>164.14</td>\n",
       "      <td>10845.82</td>\n",
       "      <td>10103.35</td>\n",
       "      <td>742.47</td>\n",
       "      <td>0.0</td>\n",
       "      <td>conventional</td>\n",
       "      <td>2015</td>\n",
       "      <td>Albany</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>2015-02-01</td>\n",
       "      <td>0.99</td>\n",
       "      <td>70873.60</td>\n",
       "      <td>1353.90</td>\n",
       "      <td>60017.20</td>\n",
       "      <td>179.32</td>\n",
       "      <td>9323.18</td>\n",
       "      <td>9170.82</td>\n",
       "      <td>152.36</td>\n",
       "      <td>0.0</td>\n",
       "      <td>conventional</td>\n",
       "      <td>2015</td>\n",
       "      <td>Albany</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         Date  AveragePrice  Total Volume     4046      4225    4770  \\\n",
       "51 2015-01-04  1.22          40873.28      2819.50  28287.42  49.90    \n",
       "50 2015-01-11  1.24          41195.08      1002.85  31640.34  127.12   \n",
       "49 2015-01-18  1.17          44511.28      914.14   31540.32  135.77   \n",
       "48 2015-01-25  1.06          45147.50      941.38   33196.16  164.14   \n",
       "47 2015-02-01  0.99          70873.60      1353.90  60017.20  179.32   \n",
       "\n",
       "    Total Bags  Small Bags  Large Bags  XLarge Bags          type  year  \\\n",
       "51  9716.46     9186.93     529.53      0.0          conventional  2015   \n",
       "50  8424.77     8036.04     388.73      0.0          conventional  2015   \n",
       "49  11921.05    11651.09    269.96      0.0          conventional  2015   \n",
       "48  10845.82    10103.35    742.47      0.0          conventional  2015   \n",
       "47  9323.18     9170.82     152.36      0.0          conventional  2015   \n",
       "\n",
       "    region  \n",
       "51  Albany  \n",
       "50  Albany  \n",
       "49  Albany  \n",
       "48  Albany  \n",
       "47  Albany  "
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.sort_values(by=[\"region\", \"type\", \"Date\"]).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d06319db",
   "metadata": {},
   "source": [
    "Now there is only measurement for each date. Hence timestamps for combination of region and type are seperate. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac17a187-bf66-4339-b5f4-3f79cb6948cc",
   "metadata": {},
   "source": [
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2b56b13-e2ff-45d9-b800-fb7006f92653",
   "metadata": {},
   "source": [
    "### 2.2\n",
    "rubric={points:4}\n",
    "\n",
    "In the Rain in Australia dataset, the measurements were generally equally spaced but with some exceptions. How about with this dataset? Justify your answer by referencing the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "f210c34a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Albany', 'conventional') 7 days 00:00:00\n",
      "('Albany', 'organic') 7 days 00:00:00\n",
      "('Atlanta', 'conventional') 7 days 00:00:00\n",
      "('Atlanta', 'organic') 7 days 00:00:00\n",
      "('BaltimoreWashington', 'conventional') 7 days 00:00:00\n",
      "('BaltimoreWashington', 'organic') 7 days 00:00:00\n",
      "('Boise', 'conventional') 7 days 00:00:00\n",
      "('Boise', 'organic') 7 days 00:00:00\n",
      "('Boston', 'conventional') 7 days 00:00:00\n",
      "('Boston', 'organic') 7 days 00:00:00\n",
      "('BuffaloRochester', 'conventional') 7 days 00:00:00\n",
      "('BuffaloRochester', 'organic') 7 days 00:00:00\n",
      "('California', 'conventional') 7 days 00:00:00\n",
      "('California', 'organic') 7 days 00:00:00\n",
      "('Charlotte', 'conventional') 7 days 00:00:00\n",
      "('Charlotte', 'organic') 7 days 00:00:00\n",
      "('Chicago', 'conventional') 7 days 00:00:00\n",
      "('Chicago', 'organic') 7 days 00:00:00\n",
      "('CincinnatiDayton', 'conventional') 7 days 00:00:00\n",
      "('CincinnatiDayton', 'organic') 7 days 00:00:00\n",
      "('Columbus', 'conventional') 7 days 00:00:00\n",
      "('Columbus', 'organic') 7 days 00:00:00\n",
      "('DallasFtWorth', 'conventional') 7 days 00:00:00\n",
      "('DallasFtWorth', 'organic') 7 days 00:00:00\n",
      "('Denver', 'conventional') 7 days 00:00:00\n",
      "('Denver', 'organic') 7 days 00:00:00\n",
      "('Detroit', 'conventional') 7 days 00:00:00\n",
      "('Detroit', 'organic') 7 days 00:00:00\n",
      "('GrandRapids', 'conventional') 7 days 00:00:00\n",
      "('GrandRapids', 'organic') 7 days 00:00:00\n",
      "('GreatLakes', 'conventional') 7 days 00:00:00\n",
      "('GreatLakes', 'organic') 7 days 00:00:00\n",
      "('HarrisburgScranton', 'conventional') 7 days 00:00:00\n",
      "('HarrisburgScranton', 'organic') 7 days 00:00:00\n",
      "('HartfordSpringfield', 'conventional') 7 days 00:00:00\n",
      "('HartfordSpringfield', 'organic') 7 days 00:00:00\n",
      "('Houston', 'conventional') 7 days 00:00:00\n",
      "('Houston', 'organic') 7 days 00:00:00\n",
      "('Indianapolis', 'conventional') 7 days 00:00:00\n",
      "('Indianapolis', 'organic') 7 days 00:00:00\n",
      "('Jacksonville', 'conventional') 7 days 00:00:00\n",
      "('Jacksonville', 'organic') 7 days 00:00:00\n",
      "('LasVegas', 'conventional') 7 days 00:00:00\n",
      "('LasVegas', 'organic') 7 days 00:00:00\n",
      "('LosAngeles', 'conventional') 7 days 00:00:00\n",
      "('LosAngeles', 'organic') 7 days 00:00:00\n",
      "('Louisville', 'conventional') 7 days 00:00:00\n",
      "('Louisville', 'organic') 7 days 00:00:00\n",
      "('MiamiFtLauderdale', 'conventional') 7 days 00:00:00\n",
      "('MiamiFtLauderdale', 'organic') 7 days 00:00:00\n",
      "('Midsouth', 'conventional') 7 days 00:00:00\n",
      "('Midsouth', 'organic') 7 days 00:00:00\n",
      "('Nashville', 'conventional') 7 days 00:00:00\n",
      "('Nashville', 'organic') 7 days 00:00:00\n",
      "('NewOrleansMobile', 'conventional') 7 days 00:00:00\n",
      "('NewOrleansMobile', 'organic') 7 days 00:00:00\n",
      "('NewYork', 'conventional') 7 days 00:00:00\n",
      "('NewYork', 'organic') 7 days 00:00:00\n",
      "('Northeast', 'conventional') 7 days 00:00:00\n",
      "('Northeast', 'organic') 7 days 00:00:00\n",
      "('NorthernNewEngland', 'conventional') 7 days 00:00:00\n",
      "('NorthernNewEngland', 'organic') 7 days 00:00:00\n",
      "('Orlando', 'conventional') 7 days 00:00:00\n",
      "('Orlando', 'organic') 7 days 00:00:00\n",
      "('Philadelphia', 'conventional') 7 days 00:00:00\n",
      "('Philadelphia', 'organic') 7 days 00:00:00\n",
      "('PhoenixTucson', 'conventional') 7 days 00:00:00\n",
      "('PhoenixTucson', 'organic') 7 days 00:00:00\n",
      "('Pittsburgh', 'conventional') 7 days 00:00:00\n",
      "('Pittsburgh', 'organic') 7 days 00:00:00\n",
      "('Plains', 'conventional') 7 days 00:00:00\n",
      "('Plains', 'organic') 7 days 00:00:00\n",
      "('Portland', 'conventional') 7 days 00:00:00\n",
      "('Portland', 'organic') 7 days 00:00:00\n",
      "('RaleighGreensboro', 'conventional') 7 days 00:00:00\n",
      "('RaleighGreensboro', 'organic') 7 days 00:00:00\n",
      "('RichmondNorfolk', 'conventional') 7 days 00:00:00\n",
      "('RichmondNorfolk', 'organic') 7 days 00:00:00\n",
      "('Roanoke', 'conventional') 7 days 00:00:00\n",
      "('Roanoke', 'organic') 7 days 00:00:00\n",
      "('Sacramento', 'conventional') 7 days 00:00:00\n",
      "('Sacramento', 'organic') 7 days 00:00:00\n",
      "('SanDiego', 'conventional') 7 days 00:00:00\n",
      "('SanDiego', 'organic') 7 days 00:00:00\n",
      "('SanFrancisco', 'conventional') 7 days 00:00:00\n",
      "('SanFrancisco', 'organic') 7 days 00:00:00\n",
      "('Seattle', 'conventional') 7 days 00:00:00\n",
      "('Seattle', 'organic') 7 days 00:00:00\n",
      "('SouthCarolina', 'conventional') 7 days 00:00:00\n",
      "('SouthCarolina', 'organic') 7 days 00:00:00\n",
      "('SouthCentral', 'conventional') 7 days 00:00:00\n",
      "('SouthCentral', 'organic') 7 days 00:00:00\n",
      "('Southeast', 'conventional') 7 days 00:00:00\n",
      "('Southeast', 'organic') 7 days 00:00:00\n",
      "('Spokane', 'conventional') 7 days 00:00:00\n",
      "('Spokane', 'organic') 7 days 00:00:00\n",
      "('StLouis', 'conventional') 7 days 00:00:00\n",
      "('StLouis', 'organic') 7 days 00:00:00\n",
      "('Syracuse', 'conventional') 7 days 00:00:00\n",
      "('Syracuse', 'organic') 7 days 00:00:00\n",
      "('Tampa', 'conventional') 7 days 00:00:00\n",
      "('Tampa', 'organic') 7 days 00:00:00\n",
      "('TotalUS', 'conventional') 7 days 00:00:00\n",
      "('TotalUS', 'organic') 7 days 00:00:00\n",
      "('West', 'conventional') 7 days 00:00:00\n",
      "('West', 'organic') 7 days 00:00:00\n",
      "('WestTexNewMexico', 'conventional') 7 days 00:00:00\n",
      "('WestTexNewMexico', 'organic') 7 days 00:00:00\n"
     ]
    }
   ],
   "source": [
    "for name, group in df.groupby([\"region\", \"type\"]):\n",
    "    print(\"%s %s\" % (name, group[\"Date\"].sort_values().diff().min()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1f48e7c-1394-42d7-b481-e910c9ad8986",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "d852f973",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Albany', 'conventional') 7 days 00:00:00\n",
      "('Albany', 'organic') 7 days 00:00:00\n",
      "('Atlanta', 'conventional') 7 days 00:00:00\n",
      "('Atlanta', 'organic') 7 days 00:00:00\n",
      "('BaltimoreWashington', 'conventional') 7 days 00:00:00\n",
      "('BaltimoreWashington', 'organic') 7 days 00:00:00\n",
      "('Boise', 'conventional') 7 days 00:00:00\n",
      "('Boise', 'organic') 7 days 00:00:00\n",
      "('Boston', 'conventional') 7 days 00:00:00\n",
      "('Boston', 'organic') 7 days 00:00:00\n",
      "('BuffaloRochester', 'conventional') 7 days 00:00:00\n",
      "('BuffaloRochester', 'organic') 7 days 00:00:00\n",
      "('California', 'conventional') 7 days 00:00:00\n",
      "('California', 'organic') 7 days 00:00:00\n",
      "('Charlotte', 'conventional') 7 days 00:00:00\n",
      "('Charlotte', 'organic') 7 days 00:00:00\n",
      "('Chicago', 'conventional') 7 days 00:00:00\n",
      "('Chicago', 'organic') 7 days 00:00:00\n",
      "('CincinnatiDayton', 'conventional') 7 days 00:00:00\n",
      "('CincinnatiDayton', 'organic') 7 days 00:00:00\n",
      "('Columbus', 'conventional') 7 days 00:00:00\n",
      "('Columbus', 'organic') 7 days 00:00:00\n",
      "('DallasFtWorth', 'conventional') 7 days 00:00:00\n",
      "('DallasFtWorth', 'organic') 7 days 00:00:00\n",
      "('Denver', 'conventional') 7 days 00:00:00\n",
      "('Denver', 'organic') 7 days 00:00:00\n",
      "('Detroit', 'conventional') 7 days 00:00:00\n",
      "('Detroit', 'organic') 7 days 00:00:00\n",
      "('GrandRapids', 'conventional') 7 days 00:00:00\n",
      "('GrandRapids', 'organic') 7 days 00:00:00\n",
      "('GreatLakes', 'conventional') 7 days 00:00:00\n",
      "('GreatLakes', 'organic') 7 days 00:00:00\n",
      "('HarrisburgScranton', 'conventional') 7 days 00:00:00\n",
      "('HarrisburgScranton', 'organic') 7 days 00:00:00\n",
      "('HartfordSpringfield', 'conventional') 7 days 00:00:00\n",
      "('HartfordSpringfield', 'organic') 7 days 00:00:00\n",
      "('Houston', 'conventional') 7 days 00:00:00\n",
      "('Houston', 'organic') 7 days 00:00:00\n",
      "('Indianapolis', 'conventional') 7 days 00:00:00\n",
      "('Indianapolis', 'organic') 7 days 00:00:00\n",
      "('Jacksonville', 'conventional') 7 days 00:00:00\n",
      "('Jacksonville', 'organic') 7 days 00:00:00\n",
      "('LasVegas', 'conventional') 7 days 00:00:00\n",
      "('LasVegas', 'organic') 7 days 00:00:00\n",
      "('LosAngeles', 'conventional') 7 days 00:00:00\n",
      "('LosAngeles', 'organic') 7 days 00:00:00\n",
      "('Louisville', 'conventional') 7 days 00:00:00\n",
      "('Louisville', 'organic') 7 days 00:00:00\n",
      "('MiamiFtLauderdale', 'conventional') 7 days 00:00:00\n",
      "('MiamiFtLauderdale', 'organic') 7 days 00:00:00\n",
      "('Midsouth', 'conventional') 7 days 00:00:00\n",
      "('Midsouth', 'organic') 7 days 00:00:00\n",
      "('Nashville', 'conventional') 7 days 00:00:00\n",
      "('Nashville', 'organic') 7 days 00:00:00\n",
      "('NewOrleansMobile', 'conventional') 7 days 00:00:00\n",
      "('NewOrleansMobile', 'organic') 7 days 00:00:00\n",
      "('NewYork', 'conventional') 7 days 00:00:00\n",
      "('NewYork', 'organic') 7 days 00:00:00\n",
      "('Northeast', 'conventional') 7 days 00:00:00\n",
      "('Northeast', 'organic') 7 days 00:00:00\n",
      "('NorthernNewEngland', 'conventional') 7 days 00:00:00\n",
      "('NorthernNewEngland', 'organic') 7 days 00:00:00\n",
      "('Orlando', 'conventional') 7 days 00:00:00\n",
      "('Orlando', 'organic') 7 days 00:00:00\n",
      "('Philadelphia', 'conventional') 7 days 00:00:00\n",
      "('Philadelphia', 'organic') 7 days 00:00:00\n",
      "('PhoenixTucson', 'conventional') 7 days 00:00:00\n",
      "('PhoenixTucson', 'organic') 7 days 00:00:00\n",
      "('Pittsburgh', 'conventional') 7 days 00:00:00\n",
      "('Pittsburgh', 'organic') 7 days 00:00:00\n",
      "('Plains', 'conventional') 7 days 00:00:00\n",
      "('Plains', 'organic') 7 days 00:00:00\n",
      "('Portland', 'conventional') 7 days 00:00:00\n",
      "('Portland', 'organic') 7 days 00:00:00\n",
      "('RaleighGreensboro', 'conventional') 7 days 00:00:00\n",
      "('RaleighGreensboro', 'organic') 7 days 00:00:00\n",
      "('RichmondNorfolk', 'conventional') 7 days 00:00:00\n",
      "('RichmondNorfolk', 'organic') 7 days 00:00:00\n",
      "('Roanoke', 'conventional') 7 days 00:00:00\n",
      "('Roanoke', 'organic') 7 days 00:00:00\n",
      "('Sacramento', 'conventional') 7 days 00:00:00\n",
      "('Sacramento', 'organic') 7 days 00:00:00\n",
      "('SanDiego', 'conventional') 7 days 00:00:00\n",
      "('SanDiego', 'organic') 7 days 00:00:00\n",
      "('SanFrancisco', 'conventional') 7 days 00:00:00\n",
      "('SanFrancisco', 'organic') 7 days 00:00:00\n",
      "('Seattle', 'conventional') 7 days 00:00:00\n",
      "('Seattle', 'organic') 7 days 00:00:00\n",
      "('SouthCarolina', 'conventional') 7 days 00:00:00\n",
      "('SouthCarolina', 'organic') 7 days 00:00:00\n",
      "('SouthCentral', 'conventional') 7 days 00:00:00\n",
      "('SouthCentral', 'organic') 7 days 00:00:00\n",
      "('Southeast', 'conventional') 7 days 00:00:00\n",
      "('Southeast', 'organic') 7 days 00:00:00\n",
      "('Spokane', 'conventional') 7 days 00:00:00\n",
      "('Spokane', 'organic') 7 days 00:00:00\n",
      "('StLouis', 'conventional') 7 days 00:00:00\n",
      "('StLouis', 'organic') 7 days 00:00:00\n",
      "('Syracuse', 'conventional') 7 days 00:00:00\n",
      "('Syracuse', 'organic') 7 days 00:00:00\n",
      "('Tampa', 'conventional') 7 days 00:00:00\n",
      "('Tampa', 'organic') 7 days 00:00:00\n",
      "('TotalUS', 'conventional') 7 days 00:00:00\n",
      "('TotalUS', 'organic') 7 days 00:00:00\n",
      "('West', 'conventional') 7 days 00:00:00\n",
      "('West', 'organic') 7 days 00:00:00\n",
      "('WestTexNewMexico', 'conventional') 7 days 00:00:00\n",
      "('WestTexNewMexico', 'organic') 21 days 00:00:00\n"
     ]
    }
   ],
   "source": [
    "for name, group in df.groupby([\"region\", \"type\"]):\n",
    "    print(\"%s %s\" % (name, group[\"Date\"].sort_values().diff().max()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d7f8172",
   "metadata": {},
   "source": [
    "The measurements here are equally spaced as well, just with the exception of WestTextNewMexico, organic avocados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "eda01246",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1     7 days \n",
       "106   7 days \n",
       "107   7 days \n",
       "108   7 days \n",
       "109   7 days \n",
       "       ...   \n",
       "52    7 days \n",
       "165   7 days \n",
       "48    14 days\n",
       "127   21 days\n",
       "0     NaT    \n",
       "Name: Date, Length: 166, dtype: timedelta64[ns]"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "group[\"Date\"].sort_values().reset_index(drop=True).diff().sort_values()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cdf7efa",
   "metadata": {},
   "source": [
    "A period of 14 days and 21 days in the row 48 and 127 can be observed, which are the outliers."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16dc1348-c2c6-46f6-bbdf-a77810930ac1",
   "metadata": {},
   "source": [
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3ab120c-6b2b-4dfb-8765-7367a9577482",
   "metadata": {},
   "source": [
    "### 2.3\n",
    "rubric={points:4}\n",
    "\n",
    "In the Rain is Australia dataset, each location was a different place in Australia. For this dataset, look at the names of the regions. Do you think the regions are all distinct, or are there overlapping regions? Justify your answer by referencing the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "f17e63c9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Albany', 'Atlanta', 'BaltimoreWashington', 'Boise', 'Boston',\n",
       "       'BuffaloRochester', 'California', 'Charlotte', 'Chicago',\n",
       "       'CincinnatiDayton', 'Columbus', 'DallasFtWorth', 'Denver',\n",
       "       'Detroit', 'GrandRapids', 'GreatLakes', 'HarrisburgScranton',\n",
       "       'HartfordSpringfield', 'Houston', 'Indianapolis', 'Jacksonville',\n",
       "       'LasVegas', 'LosAngeles', 'Louisville', 'MiamiFtLauderdale',\n",
       "       'Midsouth', 'Nashville', 'NewOrleansMobile', 'NewYork',\n",
       "       'Northeast', 'NorthernNewEngland', 'Orlando', 'Philadelphia',\n",
       "       'PhoenixTucson', 'Pittsburgh', 'Plains', 'Portland',\n",
       "       'RaleighGreensboro', 'RichmondNorfolk', 'Roanoke', 'Sacramento',\n",
       "       'SanDiego', 'SanFrancisco', 'Seattle', 'SouthCarolina',\n",
       "       'SouthCentral', 'Southeast', 'Spokane', 'StLouis', 'Syracuse',\n",
       "       'Tampa', 'TotalUS', 'West', 'WestTexNewMexico'], dtype=object)"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"region\"].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e16d237d",
   "metadata": {},
   "source": [
    "It does not appear that the regions here are all distinct. There seems to be a greater region which entails other small regions which independtly exist on this dataset as well. This can be observed by comparing the volumes of TotalUS with the sum of all the regions for a specific type and date which is more than TotalUS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "85336712",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "31324277.73"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.query(\"region == 'TotalUS' and type == 'conventional' and Date == '20150104'\")[\"Total Volume\"].values[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "6b9a8702",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "51730521.73"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.query(\"region != 'TotalUS' and type == 'conventional' and Date == '20150104'\")[\"Total Volume\"].sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c557cb1f-155f-4eb2-99fe-b93473ba15fd",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "331ec42a-3093-46c5-b708-ca7716f939dc",
   "metadata": {},
   "source": [
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eeb32828-c8f6-4344-90bc-7ec214e448e7",
   "metadata": {},
   "source": [
    "We will use the entire dataset despite any location-based weirdness uncovered in the previous part.\n",
    "\n",
    "We would like to forecast the avocado price, which is the `AveragePrice` column. The function below is adapted from Lecture 18, with some improvements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "87ef9e53-9170-4a0c-a249-7cf0715496b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_lag_feature(\n",
    "    df, orig_feature, lag, groupby, new_feature_name=None, clip=False\n",
    "):\n",
    "    \"\"\"\n",
    "    Creates a new feature that's a lagged version of an existing one.\n",
    "\n",
    "    NOTE: assumes df is already sorted by the time columns and has unique indices.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df : pandas.core.frame.DataFrame\n",
    "        The dataset.\n",
    "    orig_feature : str\n",
    "        The column name of the feature we're copying\n",
    "    lag : int\n",
    "        The lag; negative lag means values from the past, positive lag means values from the future\n",
    "    groupby : list\n",
    "        Column(s) to group by in case df contains multiple time series\n",
    "    new_feature_name : str\n",
    "        Override the default name of the newly created column\n",
    "    clip : bool\n",
    "        If True, remove rows with a NaN values for the new feature\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    pandas.core.frame.DataFrame\n",
    "        A new dataframe with the additional column added.\n",
    "\n",
    "    TODO: could/should simplify this function by using `df.shift()`\n",
    "    \"\"\"\n",
    "\n",
    "    if new_feature_name is None:\n",
    "        if lag < 0:\n",
    "            new_feature_name = \"%s_lag%d\" % (orig_feature, -lag)\n",
    "        else:\n",
    "            new_feature_name = \"%s_ahead%d\" % (orig_feature, lag)\n",
    "\n",
    "    new_df = df.assign(**{new_feature_name: np.nan})\n",
    "    for name, group in new_df.groupby(groupby):\n",
    "        if lag < 0:  # take values from the past\n",
    "            new_df.loc[group.index[-lag:], new_feature_name] = group.iloc[:lag][\n",
    "                orig_feature\n",
    "            ].values\n",
    "        else:  # take values from the future\n",
    "            new_df.loc[group.index[:-lag], new_feature_name] = group.iloc[lag:][\n",
    "                orig_feature\n",
    "            ].values\n",
    "\n",
    "    if clip:\n",
    "        new_df = new_df.dropna(subset=[new_feature_name])\n",
    "\n",
    "    return new_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89cbe62a-05a6-4bed-8ae9-b1bb7299c16a",
   "metadata": {},
   "source": [
    "We first sort our dataframe properly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "c0049d46-d7e2-40a4-9d41-74ce2e875a05",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>AveragePrice</th>\n",
       "      <th>Total Volume</th>\n",
       "      <th>4046</th>\n",
       "      <th>4225</th>\n",
       "      <th>4770</th>\n",
       "      <th>Total Bags</th>\n",
       "      <th>Small Bags</th>\n",
       "      <th>Large Bags</th>\n",
       "      <th>XLarge Bags</th>\n",
       "      <th>type</th>\n",
       "      <th>year</th>\n",
       "      <th>region</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2015-01-04</td>\n",
       "      <td>1.22</td>\n",
       "      <td>40873.28</td>\n",
       "      <td>2819.50</td>\n",
       "      <td>28287.42</td>\n",
       "      <td>49.90</td>\n",
       "      <td>9716.46</td>\n",
       "      <td>9186.93</td>\n",
       "      <td>529.53</td>\n",
       "      <td>0.0</td>\n",
       "      <td>conventional</td>\n",
       "      <td>2015</td>\n",
       "      <td>Albany</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2015-01-11</td>\n",
       "      <td>1.24</td>\n",
       "      <td>41195.08</td>\n",
       "      <td>1002.85</td>\n",
       "      <td>31640.34</td>\n",
       "      <td>127.12</td>\n",
       "      <td>8424.77</td>\n",
       "      <td>8036.04</td>\n",
       "      <td>388.73</td>\n",
       "      <td>0.0</td>\n",
       "      <td>conventional</td>\n",
       "      <td>2015</td>\n",
       "      <td>Albany</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2015-01-18</td>\n",
       "      <td>1.17</td>\n",
       "      <td>44511.28</td>\n",
       "      <td>914.14</td>\n",
       "      <td>31540.32</td>\n",
       "      <td>135.77</td>\n",
       "      <td>11921.05</td>\n",
       "      <td>11651.09</td>\n",
       "      <td>269.96</td>\n",
       "      <td>0.0</td>\n",
       "      <td>conventional</td>\n",
       "      <td>2015</td>\n",
       "      <td>Albany</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2015-01-25</td>\n",
       "      <td>1.06</td>\n",
       "      <td>45147.50</td>\n",
       "      <td>941.38</td>\n",
       "      <td>33196.16</td>\n",
       "      <td>164.14</td>\n",
       "      <td>10845.82</td>\n",
       "      <td>10103.35</td>\n",
       "      <td>742.47</td>\n",
       "      <td>0.0</td>\n",
       "      <td>conventional</td>\n",
       "      <td>2015</td>\n",
       "      <td>Albany</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2015-02-01</td>\n",
       "      <td>0.99</td>\n",
       "      <td>70873.60</td>\n",
       "      <td>1353.90</td>\n",
       "      <td>60017.20</td>\n",
       "      <td>179.32</td>\n",
       "      <td>9323.18</td>\n",
       "      <td>9170.82</td>\n",
       "      <td>152.36</td>\n",
       "      <td>0.0</td>\n",
       "      <td>conventional</td>\n",
       "      <td>2015</td>\n",
       "      <td>Albany</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18244</th>\n",
       "      <td>2018-02-25</td>\n",
       "      <td>1.57</td>\n",
       "      <td>18421.24</td>\n",
       "      <td>1974.26</td>\n",
       "      <td>2482.65</td>\n",
       "      <td>0.00</td>\n",
       "      <td>13964.33</td>\n",
       "      <td>13698.27</td>\n",
       "      <td>266.06</td>\n",
       "      <td>0.0</td>\n",
       "      <td>organic</td>\n",
       "      <td>2018</td>\n",
       "      <td>WestTexNewMexico</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18245</th>\n",
       "      <td>2018-03-04</td>\n",
       "      <td>1.54</td>\n",
       "      <td>17393.30</td>\n",
       "      <td>1832.24</td>\n",
       "      <td>1905.57</td>\n",
       "      <td>0.00</td>\n",
       "      <td>13655.49</td>\n",
       "      <td>13401.93</td>\n",
       "      <td>253.56</td>\n",
       "      <td>0.0</td>\n",
       "      <td>organic</td>\n",
       "      <td>2018</td>\n",
       "      <td>WestTexNewMexico</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18246</th>\n",
       "      <td>2018-03-11</td>\n",
       "      <td>1.56</td>\n",
       "      <td>22128.42</td>\n",
       "      <td>2162.67</td>\n",
       "      <td>3194.25</td>\n",
       "      <td>8.93</td>\n",
       "      <td>16762.57</td>\n",
       "      <td>16510.32</td>\n",
       "      <td>252.25</td>\n",
       "      <td>0.0</td>\n",
       "      <td>organic</td>\n",
       "      <td>2018</td>\n",
       "      <td>WestTexNewMexico</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18247</th>\n",
       "      <td>2018-03-18</td>\n",
       "      <td>1.56</td>\n",
       "      <td>15896.38</td>\n",
       "      <td>2055.35</td>\n",
       "      <td>1499.55</td>\n",
       "      <td>0.00</td>\n",
       "      <td>12341.48</td>\n",
       "      <td>12114.81</td>\n",
       "      <td>226.67</td>\n",
       "      <td>0.0</td>\n",
       "      <td>organic</td>\n",
       "      <td>2018</td>\n",
       "      <td>WestTexNewMexico</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18248</th>\n",
       "      <td>2018-03-25</td>\n",
       "      <td>1.62</td>\n",
       "      <td>15303.40</td>\n",
       "      <td>2325.30</td>\n",
       "      <td>2171.66</td>\n",
       "      <td>0.00</td>\n",
       "      <td>10806.44</td>\n",
       "      <td>10569.80</td>\n",
       "      <td>236.64</td>\n",
       "      <td>0.0</td>\n",
       "      <td>organic</td>\n",
       "      <td>2018</td>\n",
       "      <td>WestTexNewMexico</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>18249 rows × 13 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            Date  AveragePrice  Total Volume     4046      4225    4770  \\\n",
       "0     2015-01-04  1.22          40873.28      2819.50  28287.42  49.90    \n",
       "1     2015-01-11  1.24          41195.08      1002.85  31640.34  127.12   \n",
       "2     2015-01-18  1.17          44511.28      914.14   31540.32  135.77   \n",
       "3     2015-01-25  1.06          45147.50      941.38   33196.16  164.14   \n",
       "4     2015-02-01  0.99          70873.60      1353.90  60017.20  179.32   \n",
       "...          ...   ...               ...          ...       ...     ...   \n",
       "18244 2018-02-25  1.57          18421.24      1974.26  2482.65   0.00     \n",
       "18245 2018-03-04  1.54          17393.30      1832.24  1905.57   0.00     \n",
       "18246 2018-03-11  1.56          22128.42      2162.67  3194.25   8.93     \n",
       "18247 2018-03-18  1.56          15896.38      2055.35  1499.55   0.00     \n",
       "18248 2018-03-25  1.62          15303.40      2325.30  2171.66   0.00     \n",
       "\n",
       "       Total Bags  Small Bags  Large Bags  XLarge Bags          type  year  \\\n",
       "0      9716.46     9186.93     529.53      0.0          conventional  2015   \n",
       "1      8424.77     8036.04     388.73      0.0          conventional  2015   \n",
       "2      11921.05    11651.09    269.96      0.0          conventional  2015   \n",
       "3      10845.82    10103.35    742.47      0.0          conventional  2015   \n",
       "4      9323.18     9170.82     152.36      0.0          conventional  2015   \n",
       "...        ...         ...        ...      ...                   ...   ...   \n",
       "18244  13964.33    13698.27    266.06      0.0          organic       2018   \n",
       "18245  13655.49    13401.93    253.56      0.0          organic       2018   \n",
       "18246  16762.57    16510.32    252.25      0.0          organic       2018   \n",
       "18247  12341.48    12114.81    226.67      0.0          organic       2018   \n",
       "18248  10806.44    10569.80    236.64      0.0          organic       2018   \n",
       "\n",
       "                 region  \n",
       "0      Albany            \n",
       "1      Albany            \n",
       "2      Albany            \n",
       "3      Albany            \n",
       "4      Albany            \n",
       "...       ...            \n",
       "18244  WestTexNewMexico  \n",
       "18245  WestTexNewMexico  \n",
       "18246  WestTexNewMexico  \n",
       "18247  WestTexNewMexico  \n",
       "18248  WestTexNewMexico  \n",
       "\n",
       "[18249 rows x 13 columns]"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_sort = df.sort_values(by=[\"region\", \"type\", \"Date\"]).reset_index(drop=True)\n",
    "df_sort"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbaee71e-d45c-48dc-81a3-ebf7195cbea4",
   "metadata": {},
   "source": [
    "We then call `create_lag_feature`. This creates a new column in the dataset `AveragePriceNextWeek`, which is the following week's `AveragePrice`. We have set `clip=True` which means it will remove rows where the target would be missing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "90736df7-04b7-40d4-a835-e8eb899da7f3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>AveragePrice</th>\n",
       "      <th>Total Volume</th>\n",
       "      <th>4046</th>\n",
       "      <th>4225</th>\n",
       "      <th>4770</th>\n",
       "      <th>Total Bags</th>\n",
       "      <th>Small Bags</th>\n",
       "      <th>Large Bags</th>\n",
       "      <th>XLarge Bags</th>\n",
       "      <th>type</th>\n",
       "      <th>year</th>\n",
       "      <th>region</th>\n",
       "      <th>AveragePriceNextWeek</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2015-01-04</td>\n",
       "      <td>1.22</td>\n",
       "      <td>40873.28</td>\n",
       "      <td>2819.50</td>\n",
       "      <td>28287.42</td>\n",
       "      <td>49.90</td>\n",
       "      <td>9716.46</td>\n",
       "      <td>9186.93</td>\n",
       "      <td>529.53</td>\n",
       "      <td>0.0</td>\n",
       "      <td>conventional</td>\n",
       "      <td>2015</td>\n",
       "      <td>Albany</td>\n",
       "      <td>1.24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2015-01-11</td>\n",
       "      <td>1.24</td>\n",
       "      <td>41195.08</td>\n",
       "      <td>1002.85</td>\n",
       "      <td>31640.34</td>\n",
       "      <td>127.12</td>\n",
       "      <td>8424.77</td>\n",
       "      <td>8036.04</td>\n",
       "      <td>388.73</td>\n",
       "      <td>0.0</td>\n",
       "      <td>conventional</td>\n",
       "      <td>2015</td>\n",
       "      <td>Albany</td>\n",
       "      <td>1.17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2015-01-18</td>\n",
       "      <td>1.17</td>\n",
       "      <td>44511.28</td>\n",
       "      <td>914.14</td>\n",
       "      <td>31540.32</td>\n",
       "      <td>135.77</td>\n",
       "      <td>11921.05</td>\n",
       "      <td>11651.09</td>\n",
       "      <td>269.96</td>\n",
       "      <td>0.0</td>\n",
       "      <td>conventional</td>\n",
       "      <td>2015</td>\n",
       "      <td>Albany</td>\n",
       "      <td>1.06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2015-01-25</td>\n",
       "      <td>1.06</td>\n",
       "      <td>45147.50</td>\n",
       "      <td>941.38</td>\n",
       "      <td>33196.16</td>\n",
       "      <td>164.14</td>\n",
       "      <td>10845.82</td>\n",
       "      <td>10103.35</td>\n",
       "      <td>742.47</td>\n",
       "      <td>0.0</td>\n",
       "      <td>conventional</td>\n",
       "      <td>2015</td>\n",
       "      <td>Albany</td>\n",
       "      <td>0.99</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2015-02-01</td>\n",
       "      <td>0.99</td>\n",
       "      <td>70873.60</td>\n",
       "      <td>1353.90</td>\n",
       "      <td>60017.20</td>\n",
       "      <td>179.32</td>\n",
       "      <td>9323.18</td>\n",
       "      <td>9170.82</td>\n",
       "      <td>152.36</td>\n",
       "      <td>0.0</td>\n",
       "      <td>conventional</td>\n",
       "      <td>2015</td>\n",
       "      <td>Albany</td>\n",
       "      <td>0.99</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18243</th>\n",
       "      <td>2018-02-18</td>\n",
       "      <td>1.56</td>\n",
       "      <td>17597.12</td>\n",
       "      <td>1892.05</td>\n",
       "      <td>1928.36</td>\n",
       "      <td>0.00</td>\n",
       "      <td>13776.71</td>\n",
       "      <td>13553.53</td>\n",
       "      <td>223.18</td>\n",
       "      <td>0.0</td>\n",
       "      <td>organic</td>\n",
       "      <td>2018</td>\n",
       "      <td>WestTexNewMexico</td>\n",
       "      <td>1.57</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18244</th>\n",
       "      <td>2018-02-25</td>\n",
       "      <td>1.57</td>\n",
       "      <td>18421.24</td>\n",
       "      <td>1974.26</td>\n",
       "      <td>2482.65</td>\n",
       "      <td>0.00</td>\n",
       "      <td>13964.33</td>\n",
       "      <td>13698.27</td>\n",
       "      <td>266.06</td>\n",
       "      <td>0.0</td>\n",
       "      <td>organic</td>\n",
       "      <td>2018</td>\n",
       "      <td>WestTexNewMexico</td>\n",
       "      <td>1.54</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18245</th>\n",
       "      <td>2018-03-04</td>\n",
       "      <td>1.54</td>\n",
       "      <td>17393.30</td>\n",
       "      <td>1832.24</td>\n",
       "      <td>1905.57</td>\n",
       "      <td>0.00</td>\n",
       "      <td>13655.49</td>\n",
       "      <td>13401.93</td>\n",
       "      <td>253.56</td>\n",
       "      <td>0.0</td>\n",
       "      <td>organic</td>\n",
       "      <td>2018</td>\n",
       "      <td>WestTexNewMexico</td>\n",
       "      <td>1.56</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18246</th>\n",
       "      <td>2018-03-11</td>\n",
       "      <td>1.56</td>\n",
       "      <td>22128.42</td>\n",
       "      <td>2162.67</td>\n",
       "      <td>3194.25</td>\n",
       "      <td>8.93</td>\n",
       "      <td>16762.57</td>\n",
       "      <td>16510.32</td>\n",
       "      <td>252.25</td>\n",
       "      <td>0.0</td>\n",
       "      <td>organic</td>\n",
       "      <td>2018</td>\n",
       "      <td>WestTexNewMexico</td>\n",
       "      <td>1.56</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18247</th>\n",
       "      <td>2018-03-18</td>\n",
       "      <td>1.56</td>\n",
       "      <td>15896.38</td>\n",
       "      <td>2055.35</td>\n",
       "      <td>1499.55</td>\n",
       "      <td>0.00</td>\n",
       "      <td>12341.48</td>\n",
       "      <td>12114.81</td>\n",
       "      <td>226.67</td>\n",
       "      <td>0.0</td>\n",
       "      <td>organic</td>\n",
       "      <td>2018</td>\n",
       "      <td>WestTexNewMexico</td>\n",
       "      <td>1.62</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>18141 rows × 14 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            Date  AveragePrice  Total Volume     4046      4225    4770  \\\n",
       "0     2015-01-04  1.22          40873.28      2819.50  28287.42  49.90    \n",
       "1     2015-01-11  1.24          41195.08      1002.85  31640.34  127.12   \n",
       "2     2015-01-18  1.17          44511.28      914.14   31540.32  135.77   \n",
       "3     2015-01-25  1.06          45147.50      941.38   33196.16  164.14   \n",
       "4     2015-02-01  0.99          70873.60      1353.90  60017.20  179.32   \n",
       "...          ...   ...               ...          ...       ...     ...   \n",
       "18243 2018-02-18  1.56          17597.12      1892.05  1928.36   0.00     \n",
       "18244 2018-02-25  1.57          18421.24      1974.26  2482.65   0.00     \n",
       "18245 2018-03-04  1.54          17393.30      1832.24  1905.57   0.00     \n",
       "18246 2018-03-11  1.56          22128.42      2162.67  3194.25   8.93     \n",
       "18247 2018-03-18  1.56          15896.38      2055.35  1499.55   0.00     \n",
       "\n",
       "       Total Bags  Small Bags  Large Bags  XLarge Bags          type  year  \\\n",
       "0      9716.46     9186.93     529.53      0.0          conventional  2015   \n",
       "1      8424.77     8036.04     388.73      0.0          conventional  2015   \n",
       "2      11921.05    11651.09    269.96      0.0          conventional  2015   \n",
       "3      10845.82    10103.35    742.47      0.0          conventional  2015   \n",
       "4      9323.18     9170.82     152.36      0.0          conventional  2015   \n",
       "...        ...         ...        ...      ...                   ...   ...   \n",
       "18243  13776.71    13553.53    223.18      0.0          organic       2018   \n",
       "18244  13964.33    13698.27    266.06      0.0          organic       2018   \n",
       "18245  13655.49    13401.93    253.56      0.0          organic       2018   \n",
       "18246  16762.57    16510.32    252.25      0.0          organic       2018   \n",
       "18247  12341.48    12114.81    226.67      0.0          organic       2018   \n",
       "\n",
       "                 region  AveragePriceNextWeek  \n",
       "0      Albany            1.24                  \n",
       "1      Albany            1.17                  \n",
       "2      Albany            1.06                  \n",
       "3      Albany            0.99                  \n",
       "4      Albany            0.99                  \n",
       "...       ...             ...                  \n",
       "18243  WestTexNewMexico  1.57                  \n",
       "18244  WestTexNewMexico  1.54                  \n",
       "18245  WestTexNewMexico  1.56                  \n",
       "18246  WestTexNewMexico  1.56                  \n",
       "18247  WestTexNewMexico  1.62                  \n",
       "\n",
       "[18141 rows x 14 columns]"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_hastarget = create_lag_feature(\n",
    "    df_sort, \"AveragePrice\", +1, [\"region\", \"type\"], \"AveragePriceNextWeek\", clip=True\n",
    ")\n",
    "df_hastarget"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dbfa73e-48d7-49be-af1c-dba397d9f946",
   "metadata": {},
   "source": [
    "I will now split the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "8e952769-dbd1-4052-855f-2d2f2b4101d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = df_hastarget[df_hastarget[\"Date\"] <= split_date]\n",
    "test_df = df_hastarget[df_hastarget[\"Date\"] > split_date]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65fb19b2-0a17-4133-bf78-3dea9aa61526",
   "metadata": {},
   "source": [
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "848e951f-8dde-4a34-bf4e-b2c3a6270bb0",
   "metadata": {},
   "source": [
    "### 2.4 Baseline\n",
    "rubric={points:4}\n",
    "\n",
    "Let's try a baseline. Previously we used `DummyClassifier` or `DummyRegressor` as a baseline. This time, we'll do something else as a baseline: we'll assume the price stays the same from this week to next week. So, we'll set our prediction of \"AveragePriceNextWeek\" exactly equal to \"AveragePrice\", assuming no change. That is kind of like saying, \"If it's raining today then I'm guessing it will be raining tomorrow\". This simplistic approach will not get a great score but it's a good starting point for reference. If our model does worse that this, it must not be very good. \n",
    "\n",
    "Using this baseline approach, what $R^2$ do you get?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "4a397410",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8285800937261841"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r2_score(train_df[\"AveragePriceNextWeek\"], train_df[\"AveragePrice\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8369f38-926b-4428-a276-dc1719b94d50",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "2c23e3fd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7631780188583048"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r2_score(test_df[\"AveragePriceNextWeek\"], test_df[\"AveragePrice\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0d241cd-4669-46d9-bb21-1fd8006a39e8",
   "metadata": {},
   "source": [
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d0615dc-63c2-458f-844b-f17d30758ab1",
   "metadata": {},
   "source": [
    "### (Optional) 2.5 Modeling\n",
    "rubric={points:2}\n",
    "\n",
    "Now that the baseline is done, let's build some models to forecast the average avocado price a week later. Experiment with a few approachs for encoding the date. Justify the decisions you make. Which approach worked best? Report your test score and briefly discuss your results.\n",
    "\n",
    "> Because we only have 2 splits here, we need to be a bit wary of overfitting on the test set. Try not to test on it a ridiculous number of times. If you are interested in some proper ways of dealing with this, see for example sklearn's [TimeSeriesSplit](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.TimeSeriesSplit.html), which is like cross-validation for time series data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51fe2285-38d1-409a-9f4c-3f5dfd50622a",
   "metadata": {},
   "source": [
    "<br><br><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15ad9e61-e5f6-4025-8356-f48f651a8e1e",
   "metadata": {},
   "source": [
    "## Exercise 3: Short answer questions <a name=\"3\"></a>\n",
    "\n",
    "Each question is worth 2 points."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "972060d9-742d-47ae-82c5-74b258de16e7",
   "metadata": {},
   "source": [
    "### 3.1\n",
    "rubric={points:4}\n",
    "\n",
    "The following questions pertain to Lecture 18 on time series data:\n",
    "\n",
    "1. Sometimes a time series has missing time points or, worse, time points that are unequally spaced in general. Give an example of a real world situation where the time series data would have unequally spaced time points.\n",
    "2. In class we discussed two approaches to using temporal information: encoding the date as one or more features, and creating lagged versions of features. Which of these (one/other/both/neither) two approaches would struggle with unequally spaced time points? Briefly justify your answer."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a29d74a",
   "metadata": {},
   "source": [
    "1. Time series data for natural disasters such as earthquake, floods, etc. is a real world situation where the time series data would have uneuqlly spaced time points.\n",
    "\n",
    "2. Creating lagged versions of features would be a greater struggle when dealing with unequally spaced time points since the difference of measurements is not typically same time/duration between any two records, as compared to encoding the date as one or more features."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bfb63fb-80d3-4cd4-a1f0-38ed8bfe3a21",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ef053d93-f20e-417f-81ae-35edb701020c",
   "metadata": {},
   "source": [
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc3703ae-7d56-4987-b70b-4e222f1d8b15",
   "metadata": {},
   "source": [
    "### 3.2\n",
    "rubric={points:6}\n",
    "\n",
    "The following questions pertain to Lecture 19 on survival analysis. We'll consider the use case of customer churn analysis.\n",
    "\n",
    "1. What is the problem with simply labeling customers are \"churned\" or \"not churned\" and using standard supervised learning techniques, as we did in hw5?\n",
    "2. Consider customer A who just joined last week vs. customer B who has been with the service for a year. Who do you expect will leave the service first: probably customer A, probably customer B, or we don't have enough information to answer?\n",
    "3. If a customer's survival function is almost flat during a certain period, how do we interpret that?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3aef900",
   "metadata": {},
   "source": [
    "1. There will be a negative impact on the model because we would not have any idea how long after will the customers be churned if the are labelled so.\n",
    "\n",
    "2. The information provided here is quite vague to give a precition between customer A and B.\n",
    "\n",
    "3. If the survival function is amlost flat, it could mean that the customer is not changing their activation status with the service, i.e., they are likely to remain either active/ inactive during that period."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48d8507d-3e72-469b-a946-05c816be18a4",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9d8ffdd0-8c1e-4042-8414-ac8e57caf980",
   "metadata": {},
   "source": [
    "<br><br><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04cefc8e-cf76-4c27-9aa6-c560cbc0fc2b",
   "metadata": {},
   "source": [
    "### (Optional) Exercise 4 <a name=\"4\"></a>\n",
    "rubric={points:1}\n",
    "\n",
    "**Your tasks:**\n",
    "\n",
    "What is your biggest takeaway from this course? \n",
    "\n",
    "> I'm looking forward to read your answers. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2fb9e2f-a2d2-4f56-9fb4-c91492e4801b",
   "metadata": {},
   "source": [
    "<br><br><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab723dc5-4ea6-4c44-ace9-bf345bf8c120",
   "metadata": {},
   "source": [
    "## Submission instructions \n",
    "\n",
    "**PLEASE READ:** When you are ready to submit your assignment do the following:\n",
    "\n",
    "1. Run all cells in your notebook to make sure there are no errors by doing `Kernel -> Restart Kernel and Clear All Outputs` and then `Run -> Run All Cells`. \n",
    "2. Notebooks with cell execution numbers out of order or not starting from “1” will have marks deducted. Notebooks without the output displayed may not be graded at all (because we need to see the output in order to grade your work).\n",
    "3. Upload the assignment using Gradescope's drag and drop tool. Check out this [Gradescope Student Guide](https://lthub.ubc.ca/guides/gradescope-student-guide/) if you need help with Gradescope submission. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b4e160c-d947-4123-8e67-fa3c89c9aa8f",
   "metadata": {},
   "source": [
    "### Congratulations on finishing all homework assignments!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15f3bee4-0171-4465-838f-e5ac8a943e10",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image\n",
    "\n",
    "Image(\"eva-congrats.png\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:cpsc330]",
   "language": "python",
   "name": "conda-env-cpsc330-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
